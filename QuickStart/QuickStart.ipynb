{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quick Start Guide\n",
    "\n",
    "This guide assumes that you have already downloaded the pre-processed version of the dataset and unpacked it to `data_path`. Note that this dataset is a selected subset of the entire FACT dataset.\n",
    "\n",
    "This guide uses PyTorch to run an example model on the data. For Keras & TensorFlow starting points see the files in `FACT/Benchmark`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = './data'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We begin by loading the pandas DataFrames of the training, OEIS testing, and synthetic testing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "data_training = pd.read_pickle(os.path.join(data_path, 'dataset.pickle'))\n",
    "data_test_oeis = pd.read_pickle(os.path.join(data_path, 'testset_oeis.pickle'))\n",
    "data_test_synth = pd.read_pickle(os.path.join(data_path, 'testset_synth.pickle'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can now easily inspect the structure of the data with `DataFrame.columns` and `DataFrame.head()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_training.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the purpose of this quick-start guide we will train a simple transformer without layer normalisation for a classification task. The inputs to our model shall be simply the integer sequences, and the outputs will a vector of beliefs between `0` and `1` that the given sequence belongs to a given class (such as `trigonometric` or `modulo`) or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_sequences = []\n",
    "\n",
    "for sequence in data_training['sequence']:\n",
    "\tsequence_as_list = [ int(number) for number in sequence.split(',') ]\n",
    "\tlist_of_sequences.append(sequence_as_list)\n",
    "\n",
    "x = np.array(list_of_sequences, dtype=np.float64)\n",
    "x = x[:,:,np.newaxis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = data_training[['eval_trigonometric', 'eval_polynomial',\n",
    "       'eval_exponential', 'eval_periodic', 'eval_finite', 'eval_modulo',\n",
    "       'eval_prime', 'eval_bounded', 'eval_increasing', 'eval_unique']].to_numpy(dtype=np.float64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, the 6th sequence in our dataset is a unique polynomial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y[5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use a custom wrapper (a sub-class of `torch.utils.data.Dataset`) to serve the dataset through dataloaders to the training loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data import IntegerDataset\n",
    "training_dataset = IntegerDataset(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use a transformer encoder complemented by a layer of classification sigmoids. These are described in detail in `transformer.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformer import TransformerEncoder, EncoderClassifier\n",
    "import curriculum\n",
    "\n",
    "encoder = TransformerEncoder(\n",
    "\tnum_layers = 4,\n",
    "\tdim_model = 1,\n",
    "\tnum_heads = 8,\n",
    "\tdim_feedforward = 256,\n",
    "\tdropout = 0.0,\n",
    ")\n",
    "model = EncoderClassifier(encoder, dim_model=50, num_classes=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We shall train our model on all training data available for 70 epochs with the AdamW optimizer. Simply run the cell below to start the training and see the intermediate results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "\n",
    "meta_config = {\n",
    "\t\"wandbosity\": 0,\n",
    "\t\"verbosity\": 2,\n",
    "\n",
    "\t\"optimizer\": \"adamw\",\n",
    "\t\"lr\": 0.009\n",
    "}\n",
    "meta_config = namedtuple(\"meta_config\", meta_config.keys())(*meta_config.values())\n",
    "\n",
    "# make data\n",
    "# dataloader_kwargs = {'num_workers': 1, 'pin_memory': True} if device==\"cuda\" else {}\n",
    "dataloader_kwargs = {}\n",
    "training_dataloader = torch.utils.data.DataLoader(training_dataset, batch_size=128, shuffle=True, **dataloader_kwargs)\n",
    "\n",
    "epochs = 70\n",
    "best_training_loss = float(\"inf\")\n",
    "for t in range(epochs):\n",
    "\tprint(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "\ttraining_loss = curriculum.train(model, training_dataloader, meta_config)\n",
    "\tif training_loss < best_training_loss:\n",
    "\t\ttorch.save(model.state_dict(), \"bestModel.pt\")\n",
    "print(\"Done!\")\n",
    "\n",
    "model.load_state_dict(torch.load(\"bestModel.pt\"))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's it! You have now trained a transformer-encoder classifier on the FACT dataset. We hope that this guide will serve you as a starting point for working with FACT."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('py39torch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ee2347e99b5fc766421d65e1b195366168b0cbec4e3e49c395b081a461456477"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
